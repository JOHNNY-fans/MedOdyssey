{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "model2maxlen = {\n",
    "    'gpt-4-turbo-2024-04-09': 128000,\n",
    "    'gpt-4o': 128000,\n",
    "    'claude-3-haiku-20240307': 200000,\n",
    "    'claude-3-sonnet-20240229': 200000,\n",
    "    'moonshot-v1-128k': 128000,\n",
    "    'chatglm3-6b-128k': 128000,\n",
    "    'internlm2-chat-7b': 200000,\n",
    "    'internlm2-chat-20b': 200000,\n",
    "    'Yarn-Mistral-7b-128k': 128000,\n",
    "    'Yi-6B-200K': 200000,\n",
    "}\n",
    "SAMPLE_SIZE = [4000, 8000, 16000, 32000, 64000, 128000, 200000]\n",
    "TASK_NAME_LIST = ['zh_norm', 'en_norm', 'zh_kg', 'en_kg', 'zh_table', 'zh_medcase']\n",
    "BASE_DIR = '../../evaluation_result/query_result'\n",
    "result_files = glob.glob(BASE_DIR + '/*')\n",
    "\n",
    "result_file_list = [result_file.split('/')[-1] for result_file in result_files]\n",
    "\n",
    "task_model_size_score = {task: {model: {int(size): -1 for size in SAMPLE_SIZE} for model in model2maxlen} for task in TASK_NAME_LIST}\n",
    "task_model_size_ssm_socre = {task: {model: {int(size): [] for size in SAMPLE_SIZE} for model in model2maxlen} for task in TASK_NAME_LIST}\n",
    "\n",
    "task_model_size_result = {task: {model: {int(size): [] for size in SAMPLE_SIZE} for model in model2maxlen} for task in TASK_NAME_LIST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in TASK_NAME_LIST:\n",
    "    for result_file in result_files:\n",
    "        if task in result_file.split('/')[-1]:\n",
    "            with open(result_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    result = json.loads(line)\n",
    "                    task_model_size_result[task][result['model']][int(result['sample_size'])].append({\n",
    "                        'id': result['id'],\n",
    "                        'type': result['type'],\n",
    "                        'true_answer': result['true_answer'],\n",
    "                        'pred_answer': result['pred_answer'],\n",
    "                        'pred_origin_answer': result['pred_origin'],\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm Acc\n",
    "for task in ['zh_norm', 'en_norm']:\n",
    "    for model in task_model_size_result[task]:\n",
    "        for size in task_model_size_result[task][model]:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            ssm_correct = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                total += 1\n",
    "                true_answer = result['true_answer']\n",
    "                pred_answer = result['pred_answer']\n",
    "                pred_origin_answer = result['pred_origin_answer']\n",
    "                if pred_answer == []:\n",
    "                    pred_answer = ''\n",
    "\n",
    "                if true_answer.lower() == pred_answer.lower():\n",
    "                    correct += 1\n",
    "                if true_answer.lower() in pred_answer.lower():\n",
    "                    ssm_correct += 1\n",
    "            task_model_size_score[task][model][size] = (float(correct) / float(total)) if total > 0 else -1\n",
    "            task_model_size_ssm_socre[task][model][size] = (float(ssm_correct) / float(total)) if total > 0 else -1\n",
    "        \n",
    "# KG Precision/Recall/F1\n",
    "for task in ['zh_kg', 'en_kg']:\n",
    "    for model in task_model_size_result[task]:\n",
    "        for size in task_model_size_result[task][model]:\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            FN = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                true_answer = [t.lower() for t in result['true_answer']]\n",
    "                pred_answer = [p.lower() for p in result['pred_answer']]\n",
    "                true_answer = set(true_answer)\n",
    "                pred_answer = set(pred_answer)\n",
    "                for t in true_answer:\n",
    "                    if t in pred_answer:\n",
    "                        TP += 1\n",
    "                    else:\n",
    "                        FN += 1\n",
    "                for p in pred_answer:\n",
    "                    if p not in true_answer:\n",
    "                        FP += 1\n",
    "            precision = TP / (TP + FP) if TP + FP > 0 else -1\n",
    "            recall = TP / (TP + FN) if TP + FN > 0 else -1\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else -1\n",
    "            task_model_size_score[task][model][size] = (precision, recall, f1)\n",
    "\n",
    "            total = 0\n",
    "            ssm_correct = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                total += 1\n",
    "                true_answer = [t.lower() for t in result['true_answer']]\n",
    "                true_answer = set(true_answer)\n",
    "                pred_origin_answer = result['pred_origin_answer'].lower()\n",
    "                right_answer = True\n",
    "                for t in true_answer:\n",
    "                    if t not in pred_origin_answer:\n",
    "                        right_answer = False\n",
    "                        break\n",
    "                if right_answer:\n",
    "                    ssm_correct += 1\n",
    "\n",
    "            task_model_size_ssm_socre[task][model][size] = (float(ssm_correct) / float(total)) if total > 0 else -1\n",
    "\n",
    "# Table size version\n",
    "SPLITS = [',','，',';','；','、','+',' ']\n",
    "for task in ['zh_table']:\n",
    "    for model in task_model_size_result[task]:\n",
    "        for size in task_model_size_result[task][model]:\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            FN = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                true_answer = result['true_answer']\n",
    "                pred_answer = list(set(result['pred_answer']))\n",
    "                if len(pred_answer) == 1 and len(true_answer) > 1:\n",
    "                    for split in SPLITS:\n",
    "                        if split in true_answer[0]:\n",
    "                            true_answer = [split.join(true_answer)]\n",
    "                            break\n",
    "                true_answer = [t.replace(' ', '') for t in true_answer]\n",
    "                pred_answer = [p.replace(' ', '') for p in pred_answer]\n",
    "                true_answer = set(true_answer)\n",
    "                pred_answer = set(pred_answer)\n",
    "                \n",
    "                for t in true_answer:\n",
    "                    if t in pred_answer:\n",
    "                        TP += 1\n",
    "                    else:\n",
    "                        FN += 1\n",
    "                for p in pred_answer:\n",
    "                    if p not in true_answer:\n",
    "                        FP += 1\n",
    "            precision = TP / (TP + FP) if TP + FP > 0 else -1\n",
    "            recall = TP / (TP + FN) if TP + FN > 0 else -1\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else -1\n",
    "            task_model_size_score[task][model][size] = (precision, recall, f1)\n",
    "\n",
    "            total = 0\n",
    "            ssm_correct = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                total += 1\n",
    "                true_answer = result['true_answer']\n",
    "                pred_origin_answer = result['pred_origin_answer']\n",
    "                right_answer = True\n",
    "                for t in true_answer:\n",
    "                    if t not in pred_origin_answer:\n",
    "                        right_answer = False\n",
    "                        break\n",
    "                if right_answer:\n",
    "                    ssm_correct += 1\n",
    "            task_model_size_ssm_socre[task][model][size] = (float(ssm_correct) / float(total)) if total > 0 else -1\n",
    "\n",
    "# Medcase Acc size version\n",
    "with open('../../dataset/raw_data/zh_medcase/answer_correct.json', 'r', encoding='utf-8') as f:\n",
    "    supplement_answers = json.loads(f.read())\n",
    "supplement_answers = {int(k): v for k, v in supplement_answers.items()}\n",
    "\n",
    "SPLITS = [',','，',';','；','、','+',' ']\n",
    "for task in ['zh_medcase']:\n",
    "    for model in task_model_size_result[task]:\n",
    "        for size in task_model_size_result[task][model]:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            ssm_correct = 0\n",
    "            for result in task_model_size_result[task][model][size]:\n",
    "                total += 1\n",
    "                if result['id'] in supplement_answers:\n",
    "                    true_answers = supplement_answers[result['id']]\n",
    "                else:\n",
    "                    true_answers = [result['true_answer']]\n",
    "\n",
    "                for true_answer in true_answers:\n",
    "                    true_answer = true_answer[0]\n",
    "                    pred_answer = result['pred_answer']\n",
    "                    if pred_answer == []:\n",
    "                        pred_answer = ''\n",
    "                    elif len(pred_answer) == 1:\n",
    "                        pred_answer = pred_answer[0]\n",
    "                    elif len(pred_answer) > 1:\n",
    "                        for split in SPLITS:\n",
    "                            if split in true_answer:\n",
    "                                pred_answer = split.join(pred_answer)\n",
    "                                break\n",
    "                        if type(pred_answer) == list:\n",
    "                            pred_answer = pred_answer[0]\n",
    "                    true_answer = true_answer.replace(' ', '')\n",
    "                    pred_answer = pred_answer.replace(' ', '')\n",
    "                    if true_answer == pred_answer:\n",
    "                        correct += 1\n",
    "                        break\n",
    "                right_ssm_answer = False\n",
    "                for true_answer in true_answers:\n",
    "                    true_answer = true_answer[0]\n",
    "                    pred_origin_answer = result['pred_origin_answer']\n",
    "                    if true_answer in pred_origin_answer:\n",
    "                        right_ssm_answer = True\n",
    "                        break\n",
    "                if right_ssm_answer:\n",
    "                    ssm_correct += 1\n",
    "            task_model_size_score[task][model][size] = (float(correct) / float(total)) if total > 0 else -1\n",
    "            task_model_size_ssm_socre[task][model][size] = (float(ssm_correct) / float(total)) if total > 0 else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(json.dumps(task_model_size_score, ensure_ascii=False, indent=4))\n",
    "with open('task_result_score_samplesize.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(task_model_size_score, ensure_ascii=False, indent=4))\n",
    "\n",
    "print('\\n')\n",
    "print(json.dumps(task_model_size_ssm_socre, ensure_ascii=False, indent=4))\n",
    "with open('task_result_ssm_score_samplesize.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(task_model_size_ssm_socre, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
